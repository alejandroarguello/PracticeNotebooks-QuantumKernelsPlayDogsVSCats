{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOIwuaafXadZ6rBid0Misz9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **IMPORTS AND CHECKING OF THE GPU**"],"metadata":{"id":"8cce_9K3y1H9"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"d03vlNLWzxEA"},"outputs":[],"source":["#import PyTorch libraries\n","import os\n","%pylab inline\n","import torch\n","import torchvision\n","from torchvision import models  #Parte TL\n","from torch import nn\n","import torch.optim as optim\n","import tarfile\n","from torchvision.datasets.utils import download_url\n","from torch.utils.data import random_split\n","import zipfile\n","from google.colab import drive\n","drive.mount('/content/drive') #lo concecto con mi drive\n","\n","#for TensorBoard\n","%load_ext tensorboard\n","from torch.utils.tensorboard import SummaryWriter\n","writer = SummaryWriter()\n","\n","#Import visualization library\n","import matplotlib.pyplot as plt\n","\n","#Verify PyTorch version\n","print(torch.__version__)"]},{"cell_type":"code","source":["#Check to see if we have a GPU to use for training\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print('A {} device was detected.'.format(device))\n","\n","#Print the name of the cuda device, if detected\n","if device == 'cuda':\n","  print (torch.cuda.get_device_name(device=device))"],"metadata":{"id":"yKpWyYwNdq8f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **LOADING THE DATA**"],"metadata":{"id":"pzg4vQhQDB-1"}},{"cell_type":"code","source":["#Download the dataset\n","#dataset_url = \"https://storage.googleapis.com/kaggle-data-sets/23777/30378/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20220312%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20220312T181139Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=0581989a64ba185342b361702bb4e7461d1e16d0e44d515e83fda6e5c2827402292ed4248a7db1b3b64318912b3cadc4c7478e1beb92ff84d306cdbf7e62ae8cfaaacd685ca08a716b551d4f7ba5603f92ec27033149ee32c580b4a9fe39da7598c020f95240daee7c9d92c507088ea0269b333f27ac3dc6239f45622a53b4ed132600940b476b3314bc16ec9b3423eafc8ea6febb021cf797dfb1baa2c1b18e6ae9a98ba8e6e0ec67ee199bac88fd1bceac0fcff80d7c1fe120df9af89a142c28dcf0a0504ee199698cf9ae0de70ed82f01c7ddf352cf04f58bff5d19994cd33132e6b28655962303fcba619b67f583f8061ec33d80dfbf13c23c91d4eafb70\"\n","#url=\"https://www.kaggle.com/datasets/tongpython/cat-and-dog/download\"\n","#download_url(url, '.')"],"metadata":{"id":"IzCDeiyWi_JG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["directory = './data'\n","if not os.path.exists(directory):\n","    os.mkdir(directory)"],"metadata":{"id":"6nCtmjGK3XH0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Descomprimir el zip con el dataset\n","with zipfile.ZipFile(\"./drive/MyDrive/Colab Notebooks/QuantumKernelsPlayDogsVSCats/datasetCatsVSDogs.zip\",\"r\") as z:\n","    z.extractall(\"./data\")"],"metadata":{"id":"SO-XdyhjtCYb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_dir = './data/datasetCatsVSDogs'\n","\n","print(os.listdir(data_dir))\n","classes = os.listdir(data_dir + \"/train\")\n","print(classes)"],"metadata":{"id":"CJdRj6n_15wc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchvision.datasets import ImageFolder\n","from torchvision.transforms import transforms\n","from torchvision.utils import save_image"],"metadata":{"id":"iROcXanm3c_x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["transform = transforms.Compose(\n","    [transforms.Resize((32,32)),\n","     transforms.ToTensor()])  #transforma todas las imagenes en el mismo tamaño de 32x32 pixeles\n","\n","\n","dataAugmentation_transform = transforms.Compose([\n","     #transforms.ToPILImage(),\n","     transforms.Resize((224,224)), #las imagenes deben tener al menos tamaño 224x224\n","     transforms.RandomRotation(degrees=45),\n","     transforms.RandomHorizontalFlip(p=0.5),\n","     transforms.RandomVerticalFlip(p=0.5),\n","     transforms.RandomGrayscale(p=0.2),\n","     transforms.ToTensor(),\n","     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                 std=[0.229, 0.224, 0.225]) #necesario normalize para los models de torchvision\n","     \n","     ])\n","\n","dataset = ImageFolder(data_dir+'/train', transform=dataAugmentation_transform)\n","\n","\n","#datasetAugmented = ImageFolder(data_dir+'/train', transform= dataAugmentation_transform)\n","\n","#img_num = 0\n","#for img,label in datasetAugmented:\n","#  save_image(img, 'img'+str(img_num)+'.jpg')\n","#  img_num += 1\n","\n","\n","#dataset1= torch.utils.data.ConcatDataset([dataset, datasetAugmented])"],"metadata":{"id":"TRDjZ-4O3eYR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img, label = dataset[0]\n","print(len(dataset))\n","print(img.shape, label)\n","print(dataset.classes)\n","print(dataset)"],"metadata":{"id":"7NaTz6PR5N3b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Para imprimir un par de imagenes del dataset\n","import matplotlib\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","matplotlib.rcParams['figure.facecolor'] = '#ffffff'\n","\n","\n","def show_example(img, label):\n","    print('Label: ', dataset.classes[label], \"(\"+str(label)+\")\")\n","    plt.imshow(img.permute(1, 2, 0))"],"metadata":{"id":"6KI1KNzUGgz7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["show_example(*dataset[0])"],"metadata":{"id":"JkxefbiBGnwD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["show_example(*dataset[8000])"],"metadata":{"id":"I1QaMzX7Gxee"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **TRAINING AND VALIDATION DATASETS**"],"metadata":{"id":"oLmp_EHdOXFa"}},{"cell_type":"markdown","source":["A partir del dataset formado por todas las imagenes de train, se separa en el dataset de train (train_ds) y el dataset para validation (val_ds)"],"metadata":{"id":"7JWT7eMZz0BV"}},{"cell_type":"code","source":["#Creación del validation set\n","random_seed = 42\n","torch.manual_seed(random_seed);"],"metadata":{"id":"-Pz_tcl4Q1GS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["val_size = 2400 #aquí se ajusta el tamaño del validation set. Con 2400 estamos usando un 70/30\n","train_size = len(dataset) - val_size\n","\n","train_ds, val_ds = random_split(dataset, [train_size, val_size])\n","len(train_ds), len(val_ds)"],"metadata":{"id":"ScFJztG8OfP1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Luego el train_ds y el val_ds se separan cada uno de ellos en batches de imagenes"],"metadata":{"id":"lzM9h9Jj0CnT"}},{"cell_type":"code","source":["from torch.utils.data.dataloader import DataLoader\n","\n","batch_size=32 #aquí se ajusta el tamaño de los batch, se suele ir doblando 64, 128, 256..."],"metadata":{"id":"0S3Lp50qWYS6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#creacion del train dataloader y validation dataloader que crean los batches\n","train_dl = DataLoader(train_ds, \n","                      batch_size, \n","                      shuffle=True, \n","                      num_workers=2, \n","                      pin_memory=True)\n","val_dl = DataLoader(val_ds, \n","                    batch_size*2, \n","                    num_workers=2, \n","                    pin_memory=True) #duplicamos el batch_size para el validation dataloader porque no vamos usar gradiente para la validation por lo que solo necesitaremos la mitad de la memoria"],"metadata":{"id":"v3tfDO8TWYeh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#funcion para mostrar uno de los batches\n","from torchvision.utils import make_grid\n","\n","def show_batch(dl):\n","    for images, labels in dl:\n","        fig, ax = plt.subplots(figsize=(12, 6))\n","        ax.set_xticks([]); ax.set_yticks([])\n","        ax.imshow(make_grid(images, nrow=16).permute(1, 2, 0))\n","        break\n","\n","#mostramos uno de los batches del train_dl\n","show_batch(train_dl)"],"metadata":{"id":"G1gmzRgOYhIc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **DEFINING DE MODEL (CNN)**"],"metadata":{"id":"ot0BG_9qjYjZ"}},{"cell_type":"code","source":["#función que realiza la operación de convolution\n","def apply_kernel(image, kernel):\n","    ri, ci = image.shape       # image dimensions\n","    rk, ck = kernel.shape      # kernel dimensions\n","    ro, co = ri-rk+1, ci-ck+1  # output dimensions\n","    output = torch.zeros([ro, co])\n","    for i in range(ro): \n","        for j in range(co):\n","            output[i,j] = torch.sum(image[i:i+rk,j:j+ck] * kernel)\n","    return output"],"metadata":{"id":"3PpKyem3jKBJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#MODELO DE PRUEBA CON UNA SOLA CONVOLUTIONAL LAYER\n","simple_model = nn.Sequential(\n","    nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1), #3 canales de entrada (,G,B), 8 canales de salida, kernel de 3x3, stride de 1 y padding de 1\n","    nn.MaxPool2d(2, 2) # reduce a la mitad el alto y ancho de las imagenes\n",")"],"metadata":{"id":"HNxasTRpOVDw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for images, labels in train_dl:\n","    print('images.shape:', images.shape)\n","    out = simple_model(images)\n","    print('out.shape:', out.shape)\n","    break\n","#toma un batch de 128 imagenes, 3 canales (R,G,B) e imagenes de 128x128 pixeles y devuelve --> un batch de 128 imagenes, 8 canales e imagenes de 64x64 pixeles"],"metadata":{"id":"vHlURfeaPPot"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **TRANSFER LEARNING**"],"metadata":{"id":"unmnG1Eyo3TF"}},{"cell_type":"markdown","source":["Cargar el modelo VGG16"],"metadata":{"id":"XfNVmoX2pOCG"}},{"cell_type":"code","source":["model_vgg16 = models.vgg16(pretrained=True) #se importa el modelo ya PREENTRENADO"],"metadata":{"id":"lJkcyr8-o1-9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Explorar el modelo"],"metadata":{"id":"v18jruWzpQTM"}},{"cell_type":"code","source":["model_vgg16"],"metadata":{"id":"sXUfLStQp2xJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i, w in enumerate(model_vgg16.parameters()):\n","  print(i, w.shape, w.requires_grad) #w.requires_grad indica si ese parámetro va a ser entrenable o no\n","\n","#inicialmente todas las capas de parametros son entrenables"],"metadata":{"id":"WawX9Hm_wTF0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#model_aux = nn.Sequential(*list(model_vgg16.children())[:-1])\n","#model_aux\n","\n","for i, parameter in enumerate(model_vgg16.parameters()):\n","  parameter.requires_grad = False\n","\n","for i, w in enumerate(model_vgg16.parameters()):\n","  print(i, w.shape, w.requires_grad)"],"metadata":{"id":"UR5_HTuE7A-0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.nn.modules.activation import ReLU\n","model_vgg16.classifier[6] = nn.Sequential(\n","    nn.Linear(4096, 512),\n","    nn.ReLU(),\n","    nn.Dropout(0.4),\n","    nn.Linear(512,2)\n",") #SE MODIFICA EL CLASIFICADOR PARA QUE CLASIFIQUE SOLAMENTE 2 CLASES"],"metadata":{"id":"W4CK46q8MtLM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_vgg16"],"metadata":{"id":"8Z-l288iNl_s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i, parameter in enumerate(model_vgg16.classifier.parameters()):\n","  parameter.requires_grad = True #Se ponen a TRUE solo las capas del Classifier para que SÍ se entrenen\n","\n","for i, w in enumerate(model_vgg16.parameters()):\n","  print(i, w.shape, w.requires_grad)"],"metadata":{"id":"iAcmfVTc-M6j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Primero definimos un modelo base llamado ImageClassificationBase que contiene métodos (funciones) de ayuda para el training y validation, y que son comunmente usadas."],"metadata":{"id":"QVLDZrv8Q9ZX"}},{"cell_type":"code","source":["import torch.nn.functional as F"],"metadata":{"id":"5rxAzBpzFEqe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ImageClassificationBase(nn.Module):\n","    def training_step(self, batch): #self representa el objeto que se va a ir creando eventualmente (sería como el propio modelo)\n","        images, labels = batch \n","        out = self(images)                  # Generate predictions, se pasa el batch de images al modelo(self)\n","        loss = F.cross_entropy(out, labels) # Calculate loss       \n","        acc = accuracy(out, labels)           # Calculate accuracy\n","        #return loss\n","        return {'train_loss': loss, 'train_acc': acc}\n","    \n","    def validation_step(self, batch):\n","        images, labels = batch \n","        out = self(images)                    # Generate predictions, se pasa el batch de images al modelo(self)\n","        loss = F.cross_entropy(out, labels)   # Calculate loss\n","        acc = accuracy(out, labels)           # Calculate accuracy\n","        return {'val_loss': loss.detach(), 'val_acc': acc} #devuelve la perdida de validation y la precisión de validation             #COMENTADO PARA IMPRIMIR TRAIN_ACC EN TB\n","        \n","    def validation_epoch_end(self, outputs): #toma las perdidas y precisiones de todos los diferentes batches del validation data y los combina calculando su media y devuelve una unica perdida y precisión para todo el validation set\n","        batch_losses = [x['val_loss'] for x in outputs]\n","        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n","        batch_accs = [x['val_acc'] for x in outputs]\n","        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n","        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}                                                            #COMENTADO PARA IMPRIMIR TRAIN_ACC EN TB\n","    \n","    def epoch_end(self, epoch, result): #toma los resultados del epoch y los muestra\n","        print(\"Epoch [{}], train_loss: {:.4f}, train_acc: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n","            epoch, result['train_loss'], result['train_acc'], result['val_loss'], result['val_acc']))\n","        \n","def accuracy(outputs, labels): #esta funcion calcula la precision de la prediccion\n","    _, preds = torch.max(outputs, dim=1)\n","    return torch.tensor(torch.sum(preds == labels).item() / len(preds)) #devuelve la etiqueta(label) que más aparece y la compara con las verdaderas etiquetas"],"metadata":{"id":"_N2NwkWIRG7N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ahora creamos nuestro propio modelo que extiende el ImageClassificationBase"],"metadata":{"id":"sHS8LdpWTHYQ"}},{"cell_type":"code","source":["class CatsVSDogsCnnModel(ImageClassificationBase):\n","    def __init__(self):\n","        super().__init__()\n","        self.network = model_vgg16\n","        \n","    def forward(self, xb):\n","        return self.network(xb)"],"metadata":{"id":"tkP-4IjgTOom"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = CatsVSDogsCnnModel()\n","model"],"metadata":{"id":"UQTohZ9jo-C9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Probar el modelo con un batch de imagenes del train_dl para ver que funciona correctamente"],"metadata":{"id":"Rv7GN50-pg11"}},{"cell_type":"code","source":["for images, labels in train_dl:\n","    print('images.shape:', images.shape)\n","    out = model(images)\n","    print('out.shape:', out.shape)\n","    print('out[0]:', out[0])\n","    break\n","#devuelve esto:\n","  #images.shape: torch.Size([128, 3, 32, 32])  --> Toma como entrada un batch de 128 imagenes, de 3 canales y 32x32 pixeles\n","  #out.shape: torch.Size([128, 2]) --> la salida es un batch de 128 vectores con 2 valores cada vector\n","  #out[0]: tensor([-0.0225, -0.0190], grad_fn=<SelectBackward0>)  --> el vector 0 tiene esos valores siendo el primero la probabilidad de ser un gato y la segunda la de ser un perro"],"metadata":{"id":"0Cjb5wJcpo89"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Funciones alternativas para elegir GPU o CPU, mover los datos a la gpu..."],"metadata":{"id":"QVWO8nu7xeJt"}},{"cell_type":"code","source":["def get_default_device():\n","    \"\"\"Pick GPU if available, else CPU\"\"\"\n","    if torch.cuda.is_available():\n","        return torch.device('cuda')\n","    else:\n","        return torch.device('cpu')\n","    \n","def to_device(data, device):\n","    \"\"\"Move tensor(s) to chosen device\"\"\"\n","    if isinstance(data, (list,tuple)):\n","        return [to_device(x, device) for x in data]\n","    return data.to(device, non_blocking=True)\n","\n","class DeviceDataLoader():\n","    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n","    def __init__(self, dl, device):\n","        self.dl = dl\n","        self.device = device\n","        \n","    def __iter__(self):\n","        \"\"\"Yield a batch of data after moving it to device\"\"\"\n","        for b in self.dl: \n","            yield to_device(b, self.device)\n","\n","    def __len__(self):\n","        \"\"\"Number of batches\"\"\"\n","        return len(self.dl)"],"metadata":{"id":"M4hJ7ON8rnu2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = get_default_device()\n","device\n","#vemos que hay GPU disponible (\"cuda\")"],"metadata":{"id":"GmpzIQu7x1hU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ahora pasamos los training y validation data loaders a la gpu para pasar automaticamente los batches de datos a la gpu, y con to_device se pasa el modelo a la gpu."],"metadata":{"id":"0x-KRqhFyJWM"}},{"cell_type":"code","source":["train_dl = DeviceDataLoader(train_dl, device)\n","val_dl = DeviceDataLoader(val_dl, device)\n","to_device(model, device);"],"metadata":{"id":"yFDT8Q2VyWSr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **TRAINING THE MODEL**"],"metadata":{"id":"42iv4kQkykOG"}},{"cell_type":"markdown","source":["Vamos a definir 2 funciones: `fit` y `evaluate` para entrenar el modelo usando descenso de gradiente y evaluar su actuación en el validation set."],"metadata":{"id":"OjuMZ4K8zBkN"}},{"cell_type":"code","source":["optimizer = None"],"metadata":{"id":"octQEd1acYYd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@torch.no_grad() #indica que mientras se ejecute la funcion evaluate no se compute ningún gradiente\n","def evaluate(model, val_loader):\n","    model.eval() #informa a pytorch de que estamos evaluando el modelo por lo que no habrá randomize\n","    outputs = [model.validation_step(batch) for batch in val_loader] #obtiene batches de imagenes del val_dl y los pasa a la validation_step function que devolvera la loss de la validation\n","    return model.validation_epoch_end(outputs) #calcula la media de las loss y devuelve un unico output\n","\n","def fit(epochs, lr, model, train_loader, val_loader, opt_func): #se le pasa el numero de epochs y el optimizador que usaremos SGD (stocastic gradient descend).\n","    history = []\n","    global optimizer\n","    optimizer = opt_func(model.parameters(), lr) #el optimizador toma los model.parameters que son los weights y biases de todas las capas y los va actualizando\n","    for epoch in range(epochs): #para cada epoch va a ver una fase de training y otra de validation\n","        # Training Phase \n","        model.train() #informa a pytorch de que estamos entrenando el modelo\n","        train_losses = [] #se mantiene un seguimiento de las perdidas(losses)\n","        train_acc = [] #se mantiene un seguimiento de las perdidas(losses)\n","        for batch in train_loader: #cogemos batches de imagenes del train_dl\n","            loss = model.training_step(batch) #esta funcion esta definida en ImageClassificationBase class y devuelve la perdida(loss) para el batch que se le pasa como input\n","            train_losses.append(loss['train_loss']) #para obtener al final la loss total del epoch\n","            train_acc.append(loss['train_acc']) #para obtener al final la loss total del epoch\n","            loss['train_loss'].backward() #calcula los gradientes\n","            optimizer.step() #se aplica el decenso de gradiente con un optimizador\n","            optimizer.zero_grad() #pone a 0 los gradientes calculados en loss.backwards\n","        # Validation phase\n","        result = evaluate(model, val_loader) #llama a la funcion evaluate definida más arriba en este bloque y devuelve el validation loss y validation accuracy\n","        result['train_loss'] = torch.stack(train_losses).mean().item() #calcula la media del train_losses para el epoch entero\n","        result['train_acc'] = torch.stack(train_acc).mean().item() #calcula la media del train_acc para el epoch entero\n","        model.epoch_end(epoch, result) #imprime el numero de epoch, el training loss, validation loss y validation accuracy\n","        history.append(result) #el resultado se añade al registro de resultados anteriores\n","    return history"],"metadata":{"id":"xwhaCE0Gynx1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = to_device(CatsVSDogsCnnModel(), device) #actualizamos el modelo con el que se ha pasado a la GPU"],"metadata":{"id":"odNBIkdf2-UP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["evaluate(model, val_dl)"],"metadata":{"id":"LjG2FXQ62-cc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_epochs = 20 #vamos a entrenar con 10 epochs\n","opt_func = torch.optim.SGD #usamos la función de optimizacion SGD\n","lr = 0.0001 #learning rate general"],"metadata":{"id":"YzcY2WpZ2-rl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Entrenamos el modelo:"],"metadata":{"id":"83eNB7Sx66yF"}},{"cell_type":"code","source":["history = fit(num_epochs, lr, model, train_dl, val_dl, opt_func) #coge batches de imagenes del dataset y las pasa por el modelo, coge la salida, calcula gradientes, aplica SGD, cambiar los  weights y biases ligeramente para reducir la loss y repetir eso para todos los batches de cada epoch\n","#se puede ver que la máxima val_acc (accuracy) la alcanza con 9 epochs"],"metadata":{"id":"5BhMMdNb60GM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for n_iter in range(num_epochs):\n","    #writer.add_scalar('Loss/test', history[n_iter]['val_loss'], n_iter)\n","    #writer.add_scalar('Accuracy/test', history[n_iter]['val_acc'], n_iter)\n","    #writer.add_scalar('Loss/train', history[n_iter]['train_loss'], n_iter)\n","    #writer.add_scalar('Accuracy/train', history[n_iter]['train_acc'], n_iter)\n","    writer.add_scalars('Loss/NºEpochs', {'train_loss':history[n_iter]['train_loss'], 'val_loss':history[n_iter]['val_loss']}, n_iter)\n","    writer.add_scalars('Accuracy/NºEpochs', {'train_acc':history[n_iter]['train_acc'], 'val_acc':history[n_iter]['val_acc']}, n_iter)"],"metadata":{"id":"EbjeTRPfKMlr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Graficar las validation set accuracies para ver como el modelo mejora con los distintos epochs:"],"metadata":{"id":"eaJ5hERb9LFL"}},{"cell_type":"code","source":["def plot_accuracies(history):\n","    train_accuracies = [x['train_acc'] for x in history]\n","    val_accuracies = [x['val_acc'] for x in history]\n","    plt.plot(train_accuracies, '-bx')\n","    plt.plot(val_accuracies, '-rx')\n","    plt.xlabel('epoch')\n","    plt.ylabel('accuracy')\n","    plt.legend(['Training', 'Validation'])\n","    plt.title('Accuracy vs. No. of epochs');"],"metadata":{"id":"VzUTuug39SE8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_accuracies(history) \n","#se puede aumentar la accuracy aumentando el numero de concolutional layers o el numero de canales en cada concolutional layer."],"metadata":{"id":"-p82_Rjt9WLG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Graficar las training losses y las validation losses para ver como el modelo mejora con los distintos epochs:"],"metadata":{"id":"aEvKMBF5_jlX"}},{"cell_type":"code","source":["def plot_losses(history):\n","    train_losses = [x.get('train_loss') for x in history]\n","    val_losses = [x['val_loss'] for x in history]\n","    plt.plot(train_losses, '-bx')\n","    plt.plot(val_losses, '-rx')\n","    plt.xlabel('epoch')\n","    plt.ylabel('loss')\n","    plt.legend(['Training', 'Validation'])\n","    plt.title('Loss vs. No. of epochs');"],"metadata":{"id":"Cl2kWZZT_7c7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_losses(history)\n","#a partir del epoch nº 9 el training_loss sigue disminuyendo pero el validation_loss vuelve a aumentar, se produce overfitting"],"metadata":{"id":"I3mDZuCF_-CV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **TESTING WITH INDIVIDUAL IMAGES**"],"metadata":{"id":"gW_TyW8kB8e1"}},{"cell_type":"code","source":["transform = transforms.Compose(\n","    [transforms.Resize((224,224)),\n","     transforms.ToTensor()])  #transforma todas las imagenes en el mismo tamaño de 224x224 pixeles\n","\n","test_dataset = ImageFolder(data_dir+'/test', transform=transform)#creamos un dataset con la clase ImageFolder"],"metadata":{"id":"3o2XVArzCCsp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(test_dataset))"],"metadata":{"id":"EIJjXmbWBajV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda')"],"metadata":{"id":"awuNBCI1mMZZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Función auxiliar para hacer la predicción de una imagen:"],"metadata":{"id":"I-8mw945Cit7"}},{"cell_type":"code","source":["def predict_image(img, model):\n","    # Convert to a batch of 1\n","    xb = to_device(img.unsqueeze(0), device)\n","    # Get predictions from model\n","    yb = model(xb)\n","    # Pick index with highest probability\n","    _, preds  = torch.max(yb, dim=1)\n","    # Retrieve the class label\n","    return dataset.classes[preds[0].item()]"],"metadata":{"id":"d_bSMa7uCnvT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Pruebas con algunas imágenes:"],"metadata":{"id":"oHq7DsMeCwoU"}},{"cell_type":"code","source":["img, label = test_dataset[0]\n","plt.imshow(img.permute(1, 2, 0))\n","print('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model))"],"metadata":{"id":"F6tBdXJpCypm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img, label = test_dataset[2000]\n","plt.imshow(img.permute(1, 2, 0))\n","print('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model))"],"metadata":{"id":"pyNWzJBHDFp6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Por ultimo, miramos la loss y accuracy generales del modelo sobre el test set. Estos valores deberían ser similares a los del validation set, si no, necesitaremos un mejor validation set que sea mas similar al test set"],"metadata":{"id":"ue8Wa0WfEQ0c"}},{"cell_type":"code","source":["test_loader = DeviceDataLoader(DataLoader(test_dataset, batch_size*2), device)\n","result = evaluate(model, test_loader)\n","result"],"metadata":{"id":"iXyHwKKAEhac"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **SAVING AND LOADING THE MODEL**"],"metadata":{"id":"aiDdki-tFDP-"}},{"cell_type":"code","source":["state = {\n","        'epoch': num_epochs,\n","        'lr': lr,\n","        'model_state': model.state_dict(),\n","        'optimizer_state': optimizer.state_dict(),\n","        'history': history\n","        }"],"metadata":{"id":"rsTp-tpCcBu2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(state, 'VGG16_LastLayerRetrained_definitivo.pth') #guarda el modelo en ese archivo"],"metadata":{"id":"13zVjFPgcBom"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda')"],"metadata":{"id":"0Rn6K2ofcBiG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loaded_state = torch.load('./drive/MyDrive/Colab Notebooks/QuantumKernelsPlayDogsVSCats/VGG16_FeatureExtraction.pth', map_location=device)"],"metadata":{"id":"e7ksY0IseSut"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loaded_epoch= loaded_state[\"epoch\"]"],"metadata":{"id":"ewSdPyPnelBz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loaded_model = CatsVSDogsCnnModel() #crea un nuevo modelo"],"metadata":{"id":"UpjPLVeScBLY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loaded_optimizer = torch.optim.SGD(loaded_model.parameters(), lr =0) #crea un nuevo optimizer"],"metadata":{"id":"bxtvRogqdRHm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loaded_model.load_state_dict(loaded_state[\"model_state\"]) #carga el model_state en el nuevo modelo"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SEQNRkp_dROR","executionInfo":{"status":"ok","timestamp":1656193681201,"user_tz":-120,"elapsed":3,"user":{"displayName":"ALEJANDRO ARGÜELLO SUAREZ","userId":"11454197765598937155"}},"outputId":"2b7b024d-c3a3-4157-89b9-0d9ace965d3f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":53}]},{"cell_type":"code","source":["loaded_optimizer.load_state_dict(loaded_state[\"optimizer_state\"]) #carga el optimizer_state en el nuevo optimizer"],"metadata":{"id":"_VmDj2cKdQ_9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img, label = test_dataset[0]\n","#img, label = img.cpu(), label.cpu()\n","plt.imshow(img.permute(1, 2, 0))\n","print('Label:', dataset.classes[label], ', Predicted:', predict_image(img, loaded_model))"],"metadata":{"id":"r8d_BbGOfcoj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cdygxpmEfv2E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(model.state_dict(), 'catsVSdogs-cnn.pth') #guarda el modelo en ese archivo"],"metadata":{"id":"vKPZcxyEFGmb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model2 = to_device(CatsVSDogsCnnModel(), device) #crea un nuevo modelo 2 "],"metadata":{"id":"McmjSPK2FJOC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model2.load_state_dict(torch.load('catsVSdogs-cnn.pth')) #carga el modelo 2 con el modelo que habiamos guardado en el archivo"],"metadata":{"id":"5Q-W7NobFKwC","executionInfo":{"status":"ok","timestamp":1655049932542,"user_tz":-120,"elapsed":449,"user":{"displayName":"ALEJANDRO ARGÜELLO SUAREZ","userId":"11454197765598937155"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"98a365be-f85d-4660-dbc3-be89eb7bbb66"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":54}]},{"cell_type":"code","source":["evaluate(model2, test_loader) #comprobamos que el modelo cargado tiene la misma accuracy que anteriormente"],"metadata":{"id":"OA7kQae_FQXa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **TENSORBOARD**"],"metadata":{"id":"cE307akhkEns"}},{"cell_type":"code","source":["%tensorboard --logdir=runs"],"metadata":{"id":"u-RK9SyxkHW5"},"execution_count":null,"outputs":[]}]}